{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJTG47nCBvSxx72d69Potg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xixianghu202124213/RL/blob/main/DQN\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-2048\n",
        "!pip install numpy==1.21.6"
      ],
      "metadata": {
        "id": "l3O6vYhgv34O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb_yWZ6fKvyn",
        "outputId": "725c1b99-b7e2-4f4b-ef76-46916e7ffe0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 24 13:10:17 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   55C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|  No running processes found                                                 |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import gym\n",
        "import gym_2048\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FtoWKNhtIBH",
        "outputId": "e0ab3051-3d18-435c-adb7-7aa698b5041b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Using cuda to accelerate training"
      ],
      "metadata": {
        "id": "uhRKcAAHjLxW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    ### Deep Q-Learning Network\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.conv_a = nn.Conv2d(16, 128, kernel_size=(1,2))        # Convolutional layers\n",
        "        self.conv_b = nn.Conv2d(16, 128, kernel_size=(2,1))\n",
        "\n",
        "        self.conv_aa = nn.Conv2d(128, 128, kernel_size=(1,2))\n",
        "        self.conv_ab = nn.Conv2d(128, 128, kernel_size=(2,1))\n",
        "\n",
        "        self.conv_ba = nn.Conv2d(128, 128, kernel_size=(1,2))\n",
        "        self.conv_bb = nn.Conv2d(128, 128, kernel_size=(2,1))\n",
        "        \n",
        "        self.fc = nn.Sequential(         # Linear and Relu\n",
        "            nn.Linear(7424, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 4)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # Forward function of the layer\n",
        "        x_a = F.relu(self.conv_a(x))\n",
        "        x_b = F.relu(self.conv_b(x))\n",
        "        \n",
        "        x_aa = F.relu(self.conv_aa(x_a))\n",
        "        x_ab = F.relu(self.conv_ab(x_a))\n",
        "        \n",
        "        x_ba = F.relu(self.conv_ba(x_b))\n",
        "        x_bb = F.relu(self.conv_bb(x_b))\n",
        "        \n",
        "        sh_a = x_a.shape\n",
        "        sh_aa = x_aa.shape\n",
        "        sh_ab = x_ab.shape\n",
        "        sh_b = x_b.shape\n",
        "        sh_ba = x_ba.shape\n",
        "        sh_bb = x_bb.shape\n",
        "        \n",
        "        x_a = x_a.view(sh_a[0],sh_a[1]*sh_a[2]*sh_a[3])\n",
        "        x_aa = x_aa.view(sh_aa[0],sh_aa[1]*sh_aa[2]*sh_aa[3])\n",
        "        x_ab = x_ab.view(sh_ab[0],sh_ab[1]*sh_ab[2]*sh_ab[3])\n",
        "        x_b = x_b.view(sh_b[0],sh_b[1]*sh_b[2]*sh_b[3])\n",
        "        x_ba = x_ba.view(sh_ba[0],sh_ba[1]*sh_ba[2]*sh_ba[3])\n",
        "        x_bb = x_bb.view(sh_bb[0],sh_bb[1]*sh_bb[2]*sh_bb[3])\n",
        "        \n",
        "        concat = torch.cat((x_a,x_b,x_aa,x_ab,x_ba,x_bb),dim=1)\n",
        "        \n",
        "        output = self.fc(concat)\n",
        "        \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "AUMpU9l7Uh3p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory(object):\n",
        "  ## Class for replay buffer\n",
        "    def __init__(self, memory_size, array):\n",
        "        self.memory_size = memory_size\n",
        "        self.buffer = collections.deque(array, maxlen=self.memory_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "      # Add to buffer\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "      ## Sample min(batch_size, len(buffer)) elements for buffer\n",
        "        if batch_size > len(self.buffer):\n",
        "            batch_size = len(self.buffer)\n",
        "        indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "        return [self.buffer[i] for i in indexes]\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()"
      ],
      "metadata": {
        "id": "a-JbfIKl0UIQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_values(X):\n",
        "    # Transform input of the DQN (normalization)\n",
        "    power_mat = np.zeros(shape=(1,16,4,4),dtype=np.float32)\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            if(X[i][j]==0):\n",
        "                power_mat[0][0][i][j] = 1.0\n",
        "            else:\n",
        "                power = int(math.log(X[i][j],2))\n",
        "                power_mat[0][power][i][j] = 1.0\n",
        "    return power_mat "
      ],
      "metadata": {
        "id": "SAk2ygbiG9ah"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(n_epochs, reward_mode, online_dic, target_dic, epsilon, memory_buffer, opti,print_rate = 10):             \n",
        "    # Training the agent (we input parameters coming from previous training)\n",
        "\n",
        "    GAMMA = 0.99 \n",
        "    EXPLORE = 10000\n",
        "    INITIAL_EPSILON = 0.1\n",
        "    FINAL_EPSILON = 0.0001\n",
        "    REPLAY_MEMORY = 50000   # Size of replay buffer\n",
        "    BATCH = 16  # Length of batch extracted from buffer\n",
        "\n",
        "\n",
        "    UPDATE_STEPS = 4\n",
        "    \n",
        "    begin_learn = False\n",
        "    learn_steps = 0\n",
        "    episode_reward = 0\n",
        "    scores = []\n",
        "    max_tiles = []\n",
        "      \n",
        "    env = gym.make('2048-v0')\n",
        "    n_state = env.observation_space.shape[0]\n",
        "    n_action = env.action_space.n\n",
        "\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    memory_replay = Memory(REPLAY_MEMORY, np.array([]))\n",
        "    onlineQNetwork = DQN().to(device)\n",
        "    targetQNetwork = DQN().to(device)\n",
        "    targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "      state = env.reset()\n",
        "      episode_reward = 0\n",
        "      done = False\n",
        "        \n",
        "      while not done:\n",
        "            \n",
        "          x = change_values(state)\n",
        "          x = torch.from_numpy(np.flip(x,axis=0).copy()).to(device)\n",
        "            \n",
        "          \n",
        "          # Epsilon-greedy approach for the policy\n",
        "          if random.random() < epsilon:\n",
        "              action = random.randint(0,3)\n",
        "              next_state, reward, done, _ = env.step(action)\n",
        "              while (state == next_state).all():\n",
        "                  action = random.randint(0,3)\n",
        "                  next_state, reward, done, _ = env.step(action)             \n",
        "          else:\n",
        "              output = onlineQNetwork.forward(x) \n",
        "              for action in output.argsort()[0].cpu().numpy()[::-1]:\n",
        "                  next_state, reward, done, _ = env.step(action)\n",
        "                  if (state == next_state).all() == False:\n",
        "                      break\n",
        "\n",
        "              \n",
        "          episode_reward += reward\n",
        "          memory_replay.add((change_values(state), change_values(next_state), action, reward, done))  ## Adding data to the replay buffer\n",
        "            \n",
        "          if memory_replay.size() > 128:\n",
        "              if begin_learn is False:\n",
        "                  print('learn begin!')\n",
        "                  begin_learn = True\n",
        "              learn_steps += 1\n",
        "              if learn_steps % UPDATE_STEPS == 0:\n",
        "                  targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
        "              batch = memory_replay.sample(BATCH)\n",
        "              batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n",
        "\n",
        "              batch_state = torch.FloatTensor(batch_state).squeeze(1).to(device)\n",
        "              batch_next_state = torch.FloatTensor(batch_next_state).squeeze(1).to(device)\n",
        "              batch_action = torch.Tensor(batch_action).unsqueeze(1).to(device)\n",
        "              batch_reward = torch.Tensor(batch_reward).unsqueeze(1).to(device)\n",
        "              batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  targetQ_next = targetQNetwork(batch_next_state)\n",
        "                  y = batch_reward + (1 - batch_done) * GAMMA * torch.max(targetQ_next, dim=1, keepdim=True)[0]      # Q-learning update\n",
        "\n",
        "              loss = F.mse_loss(onlineQNetwork(batch_state).gather(1, batch_action.long()), y)\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              if epsilon > FINAL_EPSILON:\n",
        "                  epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "                    \n",
        "          state = next_state\n",
        "        \n",
        "      scores.append(episode_reward)\n",
        "      max_tiles.append(np.max(state))\n",
        "            \n",
        "      \n",
        "      if epoch % print_rate == 0:\n",
        "          env.render()\n",
        "          #save_data(onlineQNetwork, targetQNetwork, optimizer, scores, max_tiles, epsilon, memory_replay, 0, 0, final = False)  #Uncomment to save data (not useful if you punctually train the agent)\n",
        "          print(\"Game \"+str(epoch)+\", Episode reward: \"+str(episode_reward))\n",
        "\n",
        "    return(onlineQNetwork, targetQNetwork, optimizer, scores, max_tiles, epsilon, memory_replay)"
      ],
      "metadata": {
        "id": "blLRH568MctO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(onlineQNetwork, targetQNetwork, optimizer, scores, max_tiles, epsilon, memory_replay, reward, run, path = \"/content/gdrive/My Drive/2048/\", final = True):\n",
        "  ## Saves data in drive (previously mounted)\n",
        "  if final:\n",
        "    suffix = '_reward'+str(reward)+'_run' + str(run)\n",
        "  else: \n",
        "    suffix = ''\n",
        "\n",
        "  torch.save(onlineQNetwork.state_dict(), path + \"online\"+suffix)\n",
        "  torch.save(targetQNetwork.state_dict(), path+ 'target' + suffix)\n",
        "  torch.save(optimizer,path+'opti'+ suffix)\n",
        "  np.save(path+'scores' + suffix, scores)\n",
        "  np.save(path+'max_tiles' + suffix, max_tiles)\n",
        "  np.save(path+'eps' + suffix, epsilon)\n",
        "  np.save(path+'mem' + suffix, np.array(memory_replay.buffer))\n",
        "  return()"
      ],
      "metadata": {
        "id": "p4vPLasUk4eG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onlineQNetwork, targetQNetwork, optimizer, scores, max_tiles, epsilon, memory_replay = training(2000, 'scores', 'online', 'target','eps.npy', 'mem.npy', 'opti', print_rate = 100)\n",
        "#save_data(onlineQNetwork, targetQNetwork, optimizer, scores, max_tiles, epsilon, memory_replay, 1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKfAhQBalVkn",
        "outputId": "02c0d3b6-e2ce-492a-a14e-59cf78ce6b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/gym_2048/env.py:120: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  board[tile_locs] = tiles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learn begin!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128 \t64 \t32 \t2\n",
            "64 \t16 \t2 \t8\n",
            "32 \t4 \t16 \t4\n",
            "16 \t8 \t4 \t2\n",
            "Game 0, Episode reward: 1800\n",
            "8 \t2 \t16 \t4\n",
            "4 \t8 \t4 \t8\n",
            "8 \t64 \t16 \t4\n",
            "16 \t8 \t32 \t8\n",
            "Game 100, Episode reward: 660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tiles"
      ],
      "metadata": {
        "id": "hhDqR5pwlp-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J6eI_AaVMQwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gEvW-WV4OVAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HHRzHb93mj1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}